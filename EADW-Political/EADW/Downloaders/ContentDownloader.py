#!/usr/bin/python
# -*- coding: utf-8 -*-
from threading import Thread
from bs4 import BeautifulSoup
from sqlite3 import OperationalError
from EADW.Analisers.WooshEngine import WooshEngine
from bs4 import UnicodeDammit
import urllib
import re
import sqlite3
from EADW.Analisers import EntityExtract

#News Parser:
#Download the news from newsletter website and parse them. Retrieves database and parse it to a new database   
class ContentDownloader(Thread):
    
    __dbName = ""
    whoosh = ""
    __dBEntitiesLocation = "../storage/lexicon.db"
    
    def __init__(self,dbName):
        Thread.__init__(self)
        self.__dbName = dbName
        self.entityExtraction = EntityExtract.EntityExtractor()
        self.whoosh = WooshEngine()
    
    #Read from DB each entry with: url and Date
    def start(self):
        self.__conn = sqlite3.connect(self.__dbName)     
        self.__cursor = self.__conn.cursor()   
        print "Download Content Start"
        #Take a pendent processing elements snapshot
        i = 0
        newsList = []
        for row in self.__cursor.execute("Select * from newsStorage where PROCESSED='False'"):
            newsList.append([row[0],row[1]])
              
        for news in newsList:
            self.parseSite(news[0],news[1])
     
        

    ######################################################
    #####  Download site content and store it on database
    ######################################################
    def parseSite(self,url,date,justDownload=0):
        fileURL = urllib.urlopen(url)
        
        domain = re.split("http://",url)[1]        
        domain = re.split("\.pt|\.com",domain)[0]
            
        doc = fileURL.read()        
        soup = BeautifulSoup(doc)
        #print soup.original_encoding
        try:
            if domain == "expresso.sapo":
                title = unicode(soup.select("#artigo")[0].h1.get_text().encode("utf8"))
                summary = unicode(soup.select("#artigo")[0].summary.get_text().encode("utf8"))
                article =  unicode(soup.select("#conteudo")[0].get_text().encode("utf8"))
                    
            if domain == "feeds.dn":
                title = unicode(soup.select("#NewsTitle")[0].get_text().encode("utf8"))
                summary = unicode(soup.select("#NewsSummary")[0].get_text().encode("utf8"))
                article = unicode(soup.select("#Article")[0].get_text().encode("utf8"))
            
            
            if domain == "rss.feedsportal":
                title = unicode(soup.select("#NewsTitle")[0].get_text().encode("utf8"))
                summary = unicode(soup.select("#NewsSummary")[0].get_text().encode("utf8"))
                article = unicode(soup.select("#Article")[0].get_text().encode("utf8"))
            
            
            if domain == "economico.sapo":
                title = unicode(soup.select(".meta")[0].h2.get_text().encode("utf8"))
                summary = unicode(soup.select(".mainText")[0].strong.get_text().encode("utf8"))
                article = unicode(soup.select(".mainText")[0].get_text().encode("utf8"))
            
            if domain == "www.sol":
                title = unicode(soup.select("#NewsTitle")[0].get_text().encode("utf8"))
                summary = ""
                article = unicode(soup.select("#NewsSummary")[0].get_text().encode("utf8"))
                article.replace("SOL"," ")
                article.replace("SOLTags"," ")

            
            if domain == "www.rtp":
                title = unicode(soup.select("#video_detail")[0].h1.get_text().encode("utf8").encode("utf8"))
                summary = ""
                article =  unicode(soup.select("#video_detail")[0].h2.get_text().decode('utf8').encode("utf8"))
            
            if title == "":
                return;
            
            if(justDownload == 1):
                return article         
            
            self.storeNew(url,date,domain,title,summary,article);
        except IndexError:
            print "####IndexError: Ignore entry: "+url
            return
        except UnboundLocalError:
            print "####Invalid domain: "+url
            return
        #except: 
            #print "####Unexpected error: ignore entry:"+url
        
        print url
        #Sacar as entidades e guardar na base de dados das entidades e opinioes
        self.__conn.commit()     
        self.__conn.close()
        result = self.entityExtraction.ParseEntitiesFromDoc(url,title+" "+summary+" "+article)
        self.__conn = sqlite3.connect(self.__dbName)     
        self.__cursor = self.__conn.cursor()  
        self.saveOpinion(result,url)

    
    #Store the article at database and add to whoosh index
    def storeNew(self,url,date,domain,title,summary,article):
        print "Store content of " + url
        try:
            self.__cursor.execute('Update newsStorage set DOMAIN=?, TITLE=?, SUMMARY=?, ARTICLE=?, DATE=?,PROCESSED="True" where URL=?',(domain,title,summary,article,date,url))
        except sqlite3.IntegrityError:
            pass

        #Adicionar conteudo ao Whoosh Indexer
        self.whoosh.addLink(url, title, summary, article);
    
    #Resultado: {"NomeEntidade", [N_Ocorrencias, Sentimento_Acumulado]}
    def saveOpinion(self,results,url):
        cursorOpinion = self.__conn.cursor()
        cursor = self.__conn.cursor()

        for (entity,value) in results.items():
            cursor.execute('UPDATE personalities SET REPUTATION=(REPUTATION+?) where NAME=?',(value[0],unicode(entity)))
            try:
                cursorOpinion.execute('INSERT into opinion(URL,ENTITY,OPINION) values(?,?,?)',(unicode(url),unicode(entity),value[1]))
            except sqlite3.IntegrityError:
                cursorOpinion.execute('Update opinion set OPINION=? WHERE URL=? and ENTITY=?',(unicode(url),unicode(entity),value[1]))

           
