\subsection{Recolha e Armazenamento de Notícias}
\label{sec:news_storage}
\hspace{15pt}A recolha de noticias é iniciada descarregando os feeds dos principais jornais políticos portugueses: Diário de Notícias, Jornal de Notícias, Diário Económico, Sol. A biblioteca \textbf{feedparser} realiza a análise estrutural de cada um dos  feeds e extrai o URL da notícia original e a data do feed. Estes dados são guardados na base de dados.\\
Imediatamente a seguir ou à posteriori, descarregamos o conteúdo da notícia através do URL. A extracção de contéudos HTML é feita através da biblioteca \textbf{Beautiful Soup}. Não obstante, cada site tem um layout distinto pelo que tivemos de criar um HTML parser para cada um dos websites. O conteúdo extraído é separado em \textit{Título}, \textit{Sumário} e \textit{Artigo} de modo a atribuirmos relevância distinta a cada parte do texto. A entrada na base de dados é atualizada com o conteúdo e marcado como processado.\\
Estes dados serão utilizados para pesquisas e  extracção de informação/opiniões, assuntos abordados nas próximas secções.\\
Dada a ineficiência de refazer toda a base de dados descarregando o conteúdo novamente, criámos um sistema de caching de páginas web. As páginas descarregadas são guardadas em disco num ficheiro designado pelo hash do URL. No processo de arranque do sistema, é realizado, em várias threads paralelas, o hash (SHA-1)do URL e, caso o ficheiro exista, obtemos o conteúdo diretamente do disco.