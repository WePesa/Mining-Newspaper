\subsection{Otras Funcionalidades}%Dario
\hspace{15pt}Realizámos a recolha de nomes de cada um dos ministros do actual governo e marcámos as entidades como membros do governo. Além disso, fizemos parsing ao site da assembleia da republica e recolhemos todos os nomes de deputados e respectivos partidos e associamos às respectivas entidades.\\
Com base nesta contextualização das entidades, realizámos a análises entre partidos e da avaliação do governo.

\subsubsection{Análise Partidária}%Dario

\subsubsection{Avaliação do Governo}%Dario

\subsubsection{Caracterização das entidades}%Dario
\hspace{15pt}Recolhemos em cada frase os adjectivos que caracterizam as entidades dessa frase e associámos estes adjectivos à entidade. Deste modo, não só sabemos a opinião como também quais as caracteristicas mais faladas a cada uma das entidades.

\subsubsection{Evolução da opinião}%Dario
\hspace{15pt}Para cada entidade, podemos consultar o número de noticias positivas e o nº de noticias negativas e deste modo saber como é que a popularidade da entidade evoluiu ao longo do tempo.

\subsubsection{Interface de Consulta}%Dario
\hspace{15pt}A percepção sobre os dados recolhidos aumenta quando analisádos gráficamente. Para tal, criámos uma interface web de pesquisa. Esta interface foi criada no servidor Python Bottle \cite{bottle}.

\subsubsection{Mecanismo de caching}
\hspace{15pt}Durante a execução dos testes encontramos um \textit{bottleneck} no caso da ligção de Internet. O problema consistia no tempo que cada pagina demorava a descaregar. Para tal criamos mecanismo de caching de paginas e de descaregamento em paralelo. A ideia baseia-se em descaregar todas as paginas com um processo em multithread para um cache local. Isso diminuiu em um terço o tempo nessesario para processar a nossa base de dados. Por outro lado como todas as paginas permanciam em cache as seguintes leituras seriam locais. O nome dos ficheros é dado utilizando o url original e usando a função de resumo SHA-1 o que assegura a unicidade dos ficheiros e evita mecanismos complexos para a atribuição dos nomes num sistema de cache.

\subsubsection{Garbage Collector de Entidades}
\hspace{15pt}Como referido em \textbf{2.3} a nossa solução utiliza uma base de dados de nomes que provem de varias fontes, por outro lado o nosso sistema contém um mecanismo que tenta determinar entidades novas. Este tipo de abordagem gera muitos falsos positivos. No Nosso caso queremos nos focar nas entidades políticas que podem nem sempre ser asmais predominantes num artigo. Para resolver este problema criamos um mecanismo de treino. Este consiste em processar todos os artigos obtidos por forma a encontrar todas as entidades. Para cada entidade encontrada vamos manter um contador. No final para as cem entidades mais referenciadas vamos preguntar ao utilizador quais destas são realmente Entidades Politicas. As decisões tomadas pelo utilizador são guardadas em duas listas uma \textbf{'White List'} e uma \textbf{'Black List'}. A Black List é utilizada em conjunto com 'Stop Words' para filtrar o conteudo, desta forma estas palavras não são classificadas e não contam para as estatisticas das entidades. Por outro lado a White List é utilizada como filtro para o \textit{triningSet} desta forma utilizador ira sempre receber novas palavras para classificar.\\
Esta técnica permitiu retirar muitas palavras que não representavam entidades e os resultados podiam ser visto nas novas pesquisas efectuadas melhorando a precisão das nossas pesquisas. \\